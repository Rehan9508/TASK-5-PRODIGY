This code performs a detailed analysis and visualization of accident data in the United States. It systematically cleans, preprocesses, and analyzes the dataset to uncover trends, patterns, and insights related to accidents. Below is an explanation of the code's components:

1. Data Setup and Loading
Imports Required Libraries:
The code imports essential Python libraries for data manipulation (Pandas, NumPy), visualization (Seaborn, Matplotlib), and filesystem operations (os).

Dataset Loading:
Accident data is loaded using pd.read_csv, and its structure is explored by viewing column names, data types, and summary statistics.

Null Value Identification:
The code identifies columns with the highest percentage of missing values, allowing informed decisions about which columns to retain or drop.

2. Data Cleaning and Preprocessing
State-Level Filtering:
The dataset is filtered to include data specific to California (State == 'CA'), creating a subset for detailed analysis.

Data Transformation:

The ID column is transformed to extract numeric parts using regular expressions.
Columns with a high percentage of missing values are dropped.
Rows with missing values in key columns (e.g., weather conditions, temperature, wind speed) are removed to ensure the quality of the analysis.
Categorical and Numerical Separation:
Data is divided into categorical and numerical subsets for targeted analysis.

Categorical Analysis: Includes text-based fields like city names and weather conditions.
Numerical Analysis: Focuses on numeric metrics like temperature, pressure, and humidity.
3. Exploratory Data Analysis (EDA)
Correlation Analysis:
A heatmap displays correlations between numerical features to identify relationships (e.g., pressure vs. visibility).

City-Wise Accident Analysis:

Accident counts are calculated for each city, identifying top cities with the highest accident frequencies.
A bar chart visualizes the top 10 cities by accident count.
Severity Distribution:

Accidents are grouped by severity levels (1 to 4), and their distribution is shown using a pie chart.
This analysis highlights the proportion of accidents in each severity category.
4. Temporal Analysis
Date-Time Transformation:

The Start_Time and End_Time columns are converted to datetime format for time-based analysis.
Date and time components are extracted into separate columns for granular analysis.
Hourly Accident Trends:
A histogram visualizes the number of accidents occurring at different times of the day. This identifies peak hours for accidents (e.g., during morning or evening commutes).

5. Weather Conditions Analysis
Weather Impact:
The dataset examines the weather conditions at the time of accidents.
A bar chart ranks the top 20 weather conditions associated with accidents.
This analysis helps correlate adverse weather (e.g., fog, rain) with accident occurrences.
6. Geospatial Analysis
Latitude and Longitude Patterns:
Scatter plots visualize starting and ending coordinates of accidents, helping identify accident-prone zones geographically.
Joint plots analyze relationships between Start_Lat and Start_Lng as well as End_Lat and End_Lng, uncovering geospatial clustering of accidents.
7. Severity-Based Analysis
Severity by Latitude:
A scatter plot evaluates the severity of accidents relative to their geographic location (Start_Lat).
This analysis may reveal whether severe accidents are concentrated in certain areas (e.g., highways or urban centers).
Insights and Applications
The analysis identifies accident hotspots, patterns related to weather, time of day, and severity.
Insights from this project can be applied to traffic management, urban planning, and public safety initiatives.
Visualizations like pie charts, histograms, and scatter plots enhance the interpretability of findings.
By combining data cleaning, EDA, and visualization, this project provides a comprehensive understanding of accident trends and their potential causes.


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df_USA=pd.read_csv('/kaggle/input/us-accidents/US_Accidents_Dec21_updated.csv')

df_USA.head()

df_USA.columns

df_USA.dtypes.value_counts()

df_USA.shape   # USA DATASET 

# COLUMN NUMBER has highest no of null value , we cam drop that col further

df_USA.describe()

df_USA.State.unique

df1=df_USA[df_USA['State']=='CA']

df1['IDD'] = df1['ID'].astype('str').str.extractall('(\d+)').unstack().fillna('').sum(axis=1).astype(int)

df1

#df1['ID'].astype('int64')
#df1['ID'] = df1['ID'].astype(int)

df1.head()

df1.shape

df1.columns

df1.duplicated().sum()

d1f=df1.dropna(subset=['Precipitation(in)']) 

df1.shape

df1=df1.dropna(subset=['Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Direction', 'Wind_Speed(mph)',
                      'Weather_Condition'])

df1.shape

df1.isna().sum()/len(df1)*100

df1=df1.dropna(subset=['City','Sunrise_Sunset',
       'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'])

df1.isna().sum()/len(df1)*100

df1['Weather_Condition'].value_counts()

df1.Side.unique()

df_cat=df1.select_dtypes('object')
df_num=df1.select_dtypes(np.number)
df_cat=df_cat.drop('ID',axis=1)

df_cat=df1.select_dtypes('object')
col_name=[]
length=[]

for i in df_cat.columns:
    col_name.append(i)
    length.append(len(df_cat[i].unique()))
df_2=pd.DataFrame(zip(col_name,length),columns=['feature','count_of_unique_values'])
df_2

df1.drop(['Description','Zipcode','Weather_Timestamp'],axis=1,inplace=True)

del df1['Airport_Code']

df_num.columns

len(df_num.columns)

df_cat.columns

#bool_col.columns

#int_col.columns

#cat_col.head()

len(df['City'].unique())

#cat_col

#num_col_1=df.select_dtypes('number')

df_num=df1.select_dtypes(np.number)
col_name=[]
length=[]

for i in df_num.columns:
    col_name.append(i)
    length.append(len(df_num[i].unique()))
df_2=pd.DataFrame(zip(col_name,length),columns=['feature','count_of_unique_values'])
df_2

plt.figure(figsize=(15 ,9))
sns.heatmap(df_num.corr() , annot=True)

cities = df1['City'].unique()
len(cities)

accidents_by_cities = df1['City'].value_counts()
accidents_by_cities

#top 10 cities by number of accident
accidents_by_cities[:10]

fig, ax = plt.subplots(figsize=(8,5))
accidents_by_cities[:10].plot(kind='bar')
ax.set(title = 'Top 10 cities By Number of Accidents',
       xlabel = 'Cities',
       ylabel = 'Accidents Count')
plt.show()

accidents_severity = df1.groupby('Severity').count()['ID']
accidents_severity

fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(aspect="equal"))
label = [1,2,3,4]
plt.pie(accidents_severity, labels=label,
        autopct='%1.1f%%', pctdistance=0.85)
circle = plt.Circle( (0,0), 0.5, color='white')
p=plt.gcf()
p.gca().add_artist(circle)
ax.set_title("Accident by Severity",fontdict={'fontsize': 16})
plt.tight_layout()
plt.show()

df1['Start_Time'].dtypes

df1 = df1.astype({'Start_Time': 'datetime64[ns]', 'End_Time': 'datetime64[ns]'})
df1['Start_Time'].dtypes

df1['Start_Time'][2408]

df1['End_Time'][2408]

df1['start_date'] = [d.date() for d in df1['Start_Time']]
df1['start_time'] = [d.time() for d in df1['Start_Time']]

df1['end_date'] = [d.date() for d in df1['End_Time']]
df1['end_time'] = [d.time() for d in df1['End_Time']]

df1['end_time']

fig, ax = plt.subplots(figsize=(8,5))
sns.histplot(df1['Start_Time'].dt.hour, bins = 24)

plt.xlabel("Start Time")
plt.ylabel("Number of Occurence")
plt.title('Accidents Count By Time of Day')

plt.show()

fig, ax = plt.subplots(figsize=(8,5))
sns.histplot(df1['Start_Time'].dt.hour, bins = 24)

plt.xlabel("End_Time")
plt.ylabel("Number of Occurence")
plt.title('Accidents Count By Time of Day')

plt.show()

del df1['Start_Time']
del df1['End_Time']

#df.head()

%matplotlib inline
import os

fig, ax = plt.subplots(figsize=(8,5))
weather_conditions.sort_values(ascending=False)[:20].plot(kind='bar')
ax.set(title = 'Weather Conditions at Time of Accident Occurence',
       xlabel = 'Weather',
       ylabel = 'Accidents Count')
plt.show()

#df_num.head()

# shape of original data -> 2845342 rows

df.shape  # shape of data after removing null values

df_num.shape

# Accidents by order of severity (1 being lowest, and 4 being highest)

df1.groupby('Severity').count()['IDD']

# scatter plot

df_num.plot(kind='scatter', y='Start_Lat', x='Severity')

sns.jointplot(x=df_num.Start_Lat.values , y=df_num.Start_Lng.values,height=10)
plt.ylabel('Start lattitude', fontsize=12)
plt.xlabel('Start lattitude', fontsize=12)
plt.show()

sns.jointplot(x=df_num.End_Lat.values , y=df_num.End_Lng.values,height=10)
plt.ylabel('end lattitude', fontsize=12)
plt.xlabel('end longitude', fontsize=12)
plt.show()
